{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression with Titanic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPakfYNaNgwHPsS2iZiT3ku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syibrahima31/Project-with-torch/blob/main/Logistic_Regression_with_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHeXxtXanlgL"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot  as plt \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from seaborn import load_dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim "
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xs9F1jZpjMg"
      },
      "source": [
        "## PREPARE THE DATASET "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WIb-JLYnxo9"
      },
      "source": [
        "titanic = load_dataset(\"titanic\").select_dtypes([\"int64\",\"float64\"])\n",
        "\n",
        "# delete  Missing values \n",
        "titanic = titanic.dropna()\n",
        "\n",
        "\n",
        "X, y = titanic.iloc[:, 1:].values, titanic.iloc[:, 0].values\n",
        "y = y.reshape(-1,1)\n",
        "# Normalise the dataset \n",
        "\n",
        "scaler =StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P4UWCeXyhWB"
      },
      "source": [
        "class ToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, labels = sample \n",
        "    return torch.from_numpy(inputs), torch.from_numpy(labels)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCIDujG1o8wJ"
      },
      "source": [
        "class TitDataset(Dataset): \n",
        "  def __init__(self, X, y, transform= None):\n",
        "    self.X_train  = X.astype(np.float32)\n",
        "    self.y_train  = y.astype(np.float32)\n",
        "    self.n_sample = len(self.X_train)\n",
        "    self.transform = transform \n",
        "    \n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.X_train[index], self.y_train[index]\n",
        "\n",
        "    if self.transform : \n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return   self.n_sample"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsVmKgXho-ly"
      },
      "source": [
        "# use Dataloader \n",
        "dataset = TitDataset(X, y , transform=ToTensor())\n",
        "batch_size = 50\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "n_features = X.shape[1]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkt35aqU45qq"
      },
      "source": [
        "## CREATE LOGISTIC REGRESSION MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpQRXVg21zGM"
      },
      "source": [
        " class LogisticRegression(nn.Module):\n",
        "   def __init__(self, in_features, out_features=1):\n",
        "     super(LogisticRegression, self).__init__()\n",
        "     self.layer = nn.Linear(in_features, out_features)\n",
        "\n",
        "   def forward(self, x):\n",
        "    x = self.layer(x)\n",
        "    return torch.sigmoid(x)    "
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aEf1U8p9HCU"
      },
      "source": [
        "model = LogisticRegression(n_features)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUwIApSv7wjJ"
      },
      "source": [
        "## DEFINE LOSS AND OPTIMIZER "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WMMcxKA0BQ0"
      },
      "source": [
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjPj0iM-9kQ8"
      },
      "source": [
        "## TRAINING LOOP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_IuCwamq7YX",
        "outputId": "7e8623ef-fc38-4dab-f201-c7f42a366193"
      },
      "source": [
        "n_epochs = 20\n",
        "n_iterations = len(dataloader)\n",
        "L = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  \n",
        "  for i,(data, target) in enumerate(dataloader):\n",
        "    # Forward pass \n",
        "    prediction = model(data)\n",
        "   \n",
        "    # loss function \n",
        "    loss = criterion(prediction, target)\n",
        "\n",
        "    # compute the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update the weight \n",
        "    optimizer.step()\n",
        "\n",
        "    # zero gradient \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (i+ 1) % 10 ==0:\n",
        "      print(f\"epochs:{epoch}/{n_epochs}, steps:{i}/{n_iterations}, loss={loss.item()}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred_probs  = model(data)\n",
        "      pred_labels = pred_probs.round()\n",
        "      acc = pred_labels.eq(target).sum() / data.shape[0]\n",
        "      L.append(acc.item())\n",
        "\n",
        "\n"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs:0/20, steps:9/15, loss=0.6236314177513123\n",
            "epochs:1/20, steps:9/15, loss=0.67609041929245\n",
            "epochs:2/20, steps:9/15, loss=0.5678597092628479\n",
            "epochs:3/20, steps:9/15, loss=0.5792040824890137\n",
            "epochs:4/20, steps:9/15, loss=0.5958834290504456\n",
            "epochs:5/20, steps:9/15, loss=0.4794982075691223\n",
            "epochs:6/20, steps:9/15, loss=0.5449826717376709\n",
            "epochs:7/20, steps:9/15, loss=0.5852930545806885\n",
            "epochs:8/20, steps:9/15, loss=0.5348368287086487\n",
            "epochs:9/20, steps:9/15, loss=0.5403425097465515\n",
            "epochs:10/20, steps:9/15, loss=0.6452415585517883\n",
            "epochs:11/20, steps:9/15, loss=0.5748589038848877\n",
            "epochs:12/20, steps:9/15, loss=0.49865615367889404\n",
            "epochs:13/20, steps:9/15, loss=0.598423421382904\n",
            "epochs:14/20, steps:9/15, loss=0.5951711535453796\n",
            "epochs:15/20, steps:9/15, loss=0.5542836785316467\n",
            "epochs:16/20, steps:9/15, loss=0.5359903573989868\n",
            "epochs:17/20, steps:9/15, loss=0.6309725046157837\n",
            "epochs:18/20, steps:9/15, loss=0.47078341245651245\n",
            "epochs:19/20, steps:9/15, loss=0.5635089874267578\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}